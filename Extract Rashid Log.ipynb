{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "&#x1F534;\n**Author** : Manish Bhandari (Smart Dubai Govt.)\n\n**Project** : Extract Log\n\n**Crated on** : 22nd-Mar-2018\n\n\n\n----", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from cloudant.client import Cloudant\nfrom cloudant.error import CloudantException\nfrom cloudant.result import Result, ResultByKey\nfrom scipy.stats import stats\nimport cloudant as cld\nfrom dateutil import tz\nimport datetime\nimport time\nimport json\nimport csv\nimport pandas as pd\nimport numpy as np\nfrom stemming.porter2 import stem\nfrom nltk.corpus import stopwords"
        }, 
        {
            "source": "#### &#x1F534; Variables Setup", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "environment = 2  # UAT: 1, Dev : 0\nlanguage = 'EN' \nfromDateTime = datetime.datetime(2018,1,1,0,0,0) # time in GMT  Format YYYY, MM , DD , HH,MM, SS\ntoDateTime   = datetime.datetime(2018,2,28,23,59,0)\noutput_file =  'RASHID_LOG_'+language+'_'+ str(fromDateTime.strftime(\"%Y%m%d%H%M\"))+'_' + str(toDateTime.strftime(\"%Y%m%d%H%M\")) +'.csv'"
        }, 
        {
            "source": "#### &#x1F534; Cloudant DB Details", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "#### &#x1F534;Convert Date to UNIX Date format", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "epoch = datetime.datetime(1970,1,1)\nfrom_datetime = (fromDateTime - epoch).total_seconds()*1000\nto_datetime = (toDateTime - epoch).total_seconds()*1000"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from_zone = tz.gettz('UTC')\nto_zone = tz.gettz('Asia/Dubai')\ndef convertToDateTime(df , colnames):  # col name must be provided as a list\n    for col in colnames:\n        df[col] = pd.to_datetime(df[col])\n    return df\n\ndef convertUtcToLocal(df , colnames):\n    for col in colnames:\n        df[col]=df[col].dt.tz_localize('utc').dt.tz_convert(to_zone).astype(str)\n        df[col]= df[col].map(lambda x: x.replace('+04:00','0')).map(lambda x: x[0:19])\n        df[col]=  pd.to_datetime(df[col])\n    return df    "
        }, 
        {
            "source": "#### &#x1F534;Connect and extract data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The dataset has 117663 samples with 9 features each.\n"
                }
            ], 
            "source": "client = Cloudant(cloudant_db_credential['serviceUsername'], cloudant_db_credential['servicePassword'], url=cloudant_db_credential['serviceURL'])\nclient.connect()\n\n# Create an instance of the database.\nmyDatabase = client[cloudant_db]   # client.create_database(databaseName)\nif myDatabase.exists:\n    rows = []\n    result_collection = Result(myDatabase.all_docs, include_docs=True)\n    query = cld.query.Query(myDatabase, sort =[{\"_id\": \"asc\"}],\n        selector= {\"$and\": [ {\"requestTime\": {\"$gte\": from_datetime,\"$lte\": to_datetime}},{\"language\": language}]},\n        fields= [\"language\",\"conversationId\",\"request.workspace_id\",\"request.requestTime\"\n                ,\"response.intents\", \"response.input.text\",\"response.output.text\",\"response.output.nodes_visited\"]\n        )\n    for doc in query()['docs']:   # for doc in query(limit=100, skip=100)['docs']:\n        row = {}\n        try:\n            row['Language'] = doc['language']\n            row['conversation_id'] = doc['conversationId']\n            row['workspace_id'] =  doc['request']['workspace_id']\n            \n            #a= doc['request']['requestTime']\n            row['request TS'] = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(doc['request']['requestTime']/1000.0))\n            \n            \n            \n            response = doc['response']\n            \n            if len(response['intents']) > 0:\n                row['Confidence'] = response['intents'][0]['confidence']\n                row['Intent'] = response['intents'][0]['intent']\n                \n             \n            if 'text' in response['input']: \n                row['User Input'] = response['input']['text']\n                \n            for outtext in response['output']['text']:\n                  if(len(outtext)>0):\n                    row['Output'] = outtext\n            \n            for nodes_visit in response['output']['nodes_visited']:\n                  if(len(nodes_visit)>0):\n                    row['nodes_visit'] = nodes_visit                    \n            \n      \n            \n            #if 'text' in response['output']:row['Output'] = ' '.join(str(v) for v in response['output']['text'])\n\n\n            rows.append(row)\n        except Exception as e: \n             pass\n  \n    log_df = pd.DataFrame(rows,columns=['Language','workspace_id','conversation_id','request TS','User Input','Output','nodes_visit','Intent','Confidence'])\n    log_df= log_df[log_df['User Input'] != 'start']\n    #log_df.dropna(subset=['User Input'], how='all')\n    log_df = convertToDateTime(log_df , ['request TS'])\n    log_df = convertUtcToLocal(log_df , ['request TS']) \n    log_df = log_df.sort_values(['conversation_id', 'request TS'], ascending=[False, False])\n    print (\"The dataset has {} samples with {} features each.\".format(*log_df.shape))\nclient.disconnect()"
        }, 
        {
            "source": "#### &#x1F534; Filter out unwanted data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "The dataset has 11536 samples with 11 features each.\n"
                }
            ], 
            "source": "searchfor = ['!!', 'select top']\nchitchat_ws =['f8877088-a64d-479d-938e-4d22f1c19217','418dbe94-9735-4272-a18d-237bc843c570']\nlog_df = log_df[(log_df['User Input'].str.len() >0)  ] # & (log_df['Output'].str.len() >0)\nlog_df['User input stem'] = log_df['User Input'].str.lower().apply(lambda sentence :\" \".join([stem(word) for word in sentence.split(\" \")])).str.strip()\nlog_df = log_df.drop_duplicates(subset=['User input stem','Output','Confidence','Intent'], keep='first') # In case there was loop created then naive approach to creak them\nlog_df = log_df[(~log_df['User Input'].str.contains('|'.join(searchfor),na=False))]\nlog_df['relevance'] = np.where(log_df['nodes_visit'].isin(['Anything else']),'Not Trained Input',\n                               np.where((log_df['Intent'].str.len() >0) & (~ log_df['workspace_id'].isin(chitchat_ws)) ,'Trained Input',\n                                np.where((log_df['Intent'].str.len() >0) & (log_df['workspace_id'].isin(chitchat_ws)) ,'ChitChat Input','Confirmation Input')))\n\nlog_df['request TS'] =  pd.to_datetime(log_df['request TS'] )\nlog_df= log_df.sort_values(by=['conversation_id','request TS'], ascending=[True,True])\nprint (\"The dataset has {} samples with {} features each.\".format(*log_df.shape))"
        }, 
        {
            "source": "#### &#x1F534; Calculate conversation duration", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "aggr_func ={'request TS' : ['min', 'max'] }\nduration_df = log_df.copy()\nduration_df = duration_df.groupby('conversation_id').agg(aggr_func).reset_index()\nduration_df['conversation_duration'] = (duration_df['request TS']['max'] - duration_df['request TS']['min']).astype('timedelta64[s]')\nduration_df.columns = [\"conversation_id\",\"start_datetime\",\"end_datetime\",'conversation_duration']\nduration_df['request TS'] =duration_df['start_datetime']\nlog_df = pd.merge(log_df, duration_df, how='left', on=['conversation_id', 'request TS'])\n#stats.describe(log_df['conversation_duration'])"
        }, 
        {
            "source": "#### &#x1F534; Split Timestamp into Date and Time", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "log_df['conversation_date'] = log_df['request TS'].dt.date\nlog_df['conversation_time'] = log_df['request TS'].dt.time\nlog_df['conversation_year'] = log_df['request TS'].dt.year\nlog_df['conversation_month'] = log_df['request TS'].dt.month\nlog_df['conversation_week'] = log_df['request TS'].dt.weekday_name\nlog_df['conversation_day'] = log_df['request TS'].dt.day\nlog_df['conversation_hours'] = log_df['request TS'].dt.hour"
        }, 
        {
            "source": "#### &#x1F534;Sore data to IBM Cloud Object Storage(COS)\n\nIBM Cloud Object Storage(COS) provides flexible storage solution to the user and it can be accessed over HTTP using a REST API. In this notebook, we will learn how to access IBM Cloud Object Storage in python.\n\n\nhttps://www.ibm.com/cloud-computing/bluemix/node/4481\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In Watson Studio, we use project to organize resources like data, notebooks, models & connections. To easily interact with these assets now we have project-lib along with object storage APIs. Project-lib is programmatic interface to interact with your data stored in object storage. It allows you to easily access all your project assets including files, connections and metadata.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "source": "#### &#x1F534; Upload Data to Object Storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#project.save_data(data=log_df.to_csv(index=False),file_name=output_file,overwrite=True)\n#log_df = pd.read_csv(project.get_file(output_file))\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}